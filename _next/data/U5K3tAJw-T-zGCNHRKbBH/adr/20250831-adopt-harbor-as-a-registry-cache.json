{"pageProps":{"projectName":"aio-harbor-registry-cache","currentAdr":{"slug":"20250831-adopt-harbor-as-a-registry-cache","package":null,"title":"Adopt Harbor as a Registry Cache","status":"accepted","supersededBy":null,"tags":["registry","caching","container","devops"],"deciders":["Venkata Mutyala","Alanis Swanepoel"],"body":{"enhancedMdx":"\nTechnical Story: \n\n## Context and Problem Statement\n\nOur cluster bootstrap and upgrade processes depend on multiple external container registries, including Quay.io and GHCR.io. This direct dependency introduces several critical risks to our operations:\n\n1.  **Reliability:** We have experienced service outages from these upstream registries, which have caused critical failures, such as pausing cluster upgrades mid-process.\n2.  **Throttling:** We are subject to inconsistent and changing rate limits that can disrupt automated processes. The alternative, managing authentication tokens for each cluster to bypass these limits, creates significant operational overhead.\n3.  **Connectivity:** We have faced persistent network routing issues between our data centers (e.g., Hetzner) and specific registries, leading to unreliable image pulls.\n\nHow can we insulate our operations from the availability, throttling, and network instability of external container registries?\n\n## Decision Drivers\n\n- Increase the reliability and availability of container images.\n- Eliminate build failures caused by external registry rate-limiting.\n- Centralize control and management of upstream registry access.\n- Reduce operational overhead of managing registry credentials across clusters.\n\n## Considered Options\n\n- Harbor\n- Sonatype Nexus Repository\n- JFrog Artifactory\n- Cloud-hosted registry caches (e.g., AWS ECR, GCP Artifact Registry)\n- Basic Docker Registry (`registry:2` image)\n- CNCF Distribution (backend for Docker Registry)\n- A custom \"registry router\" / facade\n\n## Decision Outcome\n\nChosen option: \"Harbor\", because it is a well-supported open-source project that directly addresses our needs for reliability and centralized control, has good integration with infrastructure-as-code tools like Terraform, and offers a straightforward operational model.\n\n### Positive Consequences\n\n- No rate limits unless we specify them internally.\n- Built-in image vulnerability scanning via Trivy.\n- Supports proxying a number of upstream repositories with various caching rules.\n- Ability to manage configuration easily via Terraform and Docker Compose.\n\n### Negative Consequences\n\n- **Operational Overhead:** We are now responsible for the uptime, monitoring, and security patching of the Harbor instance.\n- **New Single Point of Failure:** The service acts as a centralized dependency; its failure will block all CI/CD pipelines and cluster provisioning.\n- **Implementation-Specific Trade-offs:** The planned immutable deployment model requires the cache to be rebuilt during maintenance, and the use of local storage makes cache data ephemeral.\n\n## Pros and Cons of the Options\n\n### Harbor\n\n-   **Good**, because it has strong support for infrastructure-as-code via Terraform.\n-   **Good**, because the Docker Compose deployment model simplifies management and upgrades for a single node.\n-   **Good**, because it includes built-in vulnerability scanning with Trivy.\n-   **Good**, because it can proxy multiple upstream repositories with flexible caching rules.\n-   **Good**, because it eliminates external rate limits, giving us full control.\n-   **Bad**, because our immutable deployment model requires the cache to be rebuilt during maintenance.\n-   **Bad**, because a high-availability (HA) deployment is complex, requiring shared storage and management of multiple stateful components like Postgres and Redis.\n\n### Sonatype Nexus Repository\n\n-   **Good**, because it is a powerful, universal artifact manager that supports many package types.\n-   **Bad**, because its broad feature set adds complexity that is not required for our specific use case of caching Kubernetes images.\n-   **Bad**, because it was not investigated in-depth due to its perceived complexity for this narrow use case.\n\n### JFrog Artifactory\n\n-   **Good**, because it is a mature and feature-rich universal artifact manager.\n-   **Bad**, because prior operational experience with the tool showed it to be complex and difficult to manage.\n\n### Cloud-hosted registry caches\n\n-   **Good**, because they are fully managed services, which would eliminate operational overhead.\n-   **Good**, because they typically offer high availability and scalability out of the box.\n-   **Bad**, because they introduce vendor lock-in to a specific cloud provider.\n-   **Bad**, because they were perceived to be a premium-cost solution compared to self-hosting.\n-   **Bad**, because they could still suffer from the same network routing issues between the cloud provider and our data centers.\n\n### Basic Docker Registry (`registry:2` image)\n\n-   **Good**, because it is lightweight, simple to run, and familiar to the team.\n-   **Bad**, because it lacks advanced features like a user interface, vulnerability scanning, and robust user management.\n-   **Bad**, because it is not designed to act as a caching proxy for multiple, disparate upstream registries.\n-   **Bad**, because it does not perform automatic garbage collection, requiring manual intervention to clean up storage when it runs out of space.\n\n### CNCF Distribution\n\n-   **Good**, because it is the foundational open-source project for Docker Registry v2 and is highly performant.\n-   **Good**, because it supports using S3 pre-signed URLs, which offloads the data transfer from the service to S3 directly.\n-   **Good**, because it supports non-AWS S3-compatible object storage providers.\n-   **Bad**, because it is primarily a library/backend, not a full-featured product, lacking a UI and other user-friendly features.\n-   **Bad**, because it is not designed to easily proxy multiple upstream registries.\n-   **Bad**, because testing revealed a critical bug where caching to S3 could fail silently, compromising the reliability of the cache.\n\n### A custom \"registry router\" / facade\n\n-   **Good**, because it could provide a single entry point for all registry requests.\n-   **Bad**, because it is not a cache; it only routes requests and does not solve the problems of upstream availability or outages.\n-   **Bad**, because it does not protect against upstream rate-limiting.\n-   **Bad**, because it is still vulnerable to the same network routing issues between our data centers and the upstream registries.\n\n## Links\n\n- [Harbor Project Website](https://goharbor.io/)\n- [Harbor Terraform Provider](https://registry.terraform.io/providers/goharbor/harbor/latest/docs)\n- [Sonatype Nexus Repository](https://www.sonatype.com/products/nexus-repository)\n- [JFrog Artifactory](https://jfrog.com/artifactory/)\n- [Docker Hub Registry Image](https://hub.docker.com/_/registry)\n- [CNCF Distribution Project (GitHub)](https://github.com/distribution/distribution/)\n- [HTTP Toolkit Blog: Docker Registry Facade](https://httptoolkit.com/blog/docker-image-registry-facade/)"},"creationDate":"2025-08-31T04:02:06.000Z","lastEditDate":"2025-08-31T04:02:06.000Z","lastEditAuthor":"Venkat","publicationDate":"2025-08-30T23:59:59.000Z","file":{"relativePath":"docs/adr/20250831-adopt-harbor-as-a-registry-cache.md","absolutePath":"/home/runner/work/aio-harbor-registry-cache/aio-harbor-registry-cache/docs/adr/20250831-adopt-harbor-as-a-registry-cache.md"},"repository":{"provider":"github","viewUrl":"https://github.com/GlueOps/aio-harbor-registry-cache/blob/master/docs/adr/20250831-adopt-harbor-as-a-registry-cache.md"}},"l4bVersion":"1.1.0"},"__N_SSG":true}
{"pageProps":{"projectName":"aio-harbor-registry-cache","currentAdr":{"slug":"20240831-harbor-standalone-deployment-pattern","package":null,"title":"Harbor Standalone Deployment Pattern","status":"accepted","supersededBy":null,"tags":["harbor","implementation","deployment-pattern","docker-compose","standalone"],"deciders":["Venkata Mutyala","Alanis Swanepoel"],"body":{"enhancedMdx":"\nTechnical Story: \n\n## Context and Problem Statement\n\nNow that Harbor has been selected, our immediate goal is to integrate it into our ecosystem efficiently, without the large upfront cost of a complex, multi-node deployment. We are adopting a \"crawl, walk, run\" approach.\n\nHow can we deploy Harbor for its initial rollout in a way that is quick to implement, simple to understand, and straightforward to manage, while establishing a foundation for potential future enhancements like high availability?\n\n## Decision Drivers\n\n- **Tiered Architecture for Resilience:** The core/replica model across data centers is a deliberate choice for availability and disaster recovery.\n- **Cache Durability and Protection:** The plan for long-lived caches on core nodes, protected from the churn of short-lived replica caches, is a key driver for ensuring upstream stability.\n- **Strict Upstream Isolation:** Forcing replicas to only talk to cores is a key security and control pattern.\n- **Predictability through Immutability:** Using an immutable model is a core operational principle that dictates the entire management lifecycle.\n- **Frictionless Bootstrapping:** The \"no auth\" decision is a specific trade-off to simplify a critical process.\n- **DNS-based Service Discovery:** Choosing DNS over LBs is a significant architectural choice for resilience, performance, and cost.\n- **Environment Parity:** Ensuring the solution works seamlessly from a developer's laptop to production is a major driver of adoption and efficiency.\n\n## Considered Options\n\n- Core/Replica Immutable Pattern (Self-hosted)\n- Simple Standalone Pattern (Self-hosted)\n- Kubernetes-based Deployment (Self-hosted)\n- Hosted Harbor as a Service (e.g., container-registry.com)\n- Traditional Mutable Server Pattern (Self-hosted)\n\n## Decision Outcome\n\nChosen option: \"**Core/Replica Immutable Pattern (Self-hosted)**\", because it best meets our drivers for resilience, cache durability, and operational predictability.\n\nThe implementation will follow a tiered architecture consisting of **Core** nodes and **Replica** nodes, both managed as immutable deployments via Terraform and Docker Compose.\n\n- **Architecture**:\n    - **Core Nodes** will run in at least two different datacenters for high availability. They will connect directly to upstream registries and maintain a long-term cache of 180 days.\n    - **Replica Nodes** will be deployed in various locations. They will be configured to use the Core nodes exclusively as their upstream proxy and will never talk to external registries. Replicas will maintain a short-term cache of 14 days.\n\n- **Networking and Availability**:\n    - A **DNS-based** approach will be used for service discovery, load balancing, and health checks (e.g., via Route53) for both core and replica pools, avoiding the need for dedicated load balancers.\n    - DNS will support geo-based routing and round-robin distribution to available nodes.\n\n- **Operations and Configuration**:\n    - **Authentication** will be disabled for image pulls to ensure a frictionless bootstrapping process for clusters.\n    - The entire setup is **immutable**; nodes will be rebuilt from scratch for any maintenance or updates.\n    - The cache on Core nodes will need to be repopulated via a manual or scheduled job after a rebuild.\n    - **Local development** will use HTTP, while CI (GitHub Actions) and production environments will use HTTPS. Production certificates will be managed manually via Let's Encrypt.\n\n### Positive Consequences\n\n- **High Resilience:** The core/replica model across multiple datacenters with DNS failover provides strong protection against single-point-of-failure outages.\n- **Cache Protection:** The tiered caching strategy protects the primary, long-term cache from being wiped out by the frequent rebuilds of ephemeral replica nodes.\n- **Consistent Environments:** The pattern works for local development, CI, and production, reducing surprises and bugs.\n\n### Negative Consequences\n\n- **Manual Cache Repopulation:** The cache on core nodes must be manually or semi-manually rebuilt after every deployment, which can be time-consuming.\n- **Manual Certificate Management:** The process for generating and deploying Let's Encrypt certificates for production nodes is a manual, operational task.\n- **Increased Architectural Complexity:** While resilient, the core/replica model is more complex to understand and troubleshoot than a single standalone instance.\n\n## Pros and Cons of the Options\n\n### Core/Replica Immutable Pattern\n\n-   **Good**, because the tiered core/replica architecture provides high resilience and protects the primary cache.\n-   **Good**, because the immutable nature of the deployment leads to highly predictable and repeatable behavior.\n-   **Good**, because using DNS for load balancing and failover is a cost-effective and scalable solution.\n-   **Good**, because the pattern is consistent across local development, CI, and production environments.\n-   **Bad**, because rebuilding the primary cache after a deployment is an operational burden.\n-   **Bad**, because it requires manual management of TLS certificates for production environments.\n-   **Bad**, because adding, removing, or changing nodes requires manual DNS record updates.\n-   **Bad**, because the overall architecture is more complex than a simple standalone deployment.\n\n### Simple Standalone Pattern\n\n-   **Good**, because the architecture is much simpler, making it easier to deploy, understand, and troubleshoot.\n-   **Good**, because there are fewer components to manage, resulting in lower operational overhead.\n-   **Bad**, because as a single point of failure, any unplanned outage or planned maintenance will result in downtime unless a complex manual DNS cutover is performed.\n-   **Bad**, because the entire cache is vulnerable and destroyed during any rebuild, increasing load on upstream registries.\n\n### Kubernetes-based Deployment\n\n-   **Good**, because Kubernetes offers automated self-healing and rolling updates, which can significantly improve availability.\n-   **Good**, because it integrates well with the cloud-native ecosystem for automating tasks like certificate management and monitoring.\n-   **Bad**, because it creates a \"chicken-and-egg\" bootstrapping problem, as the cluster itself may need the registry to deploy.\n-   **Bad**, because this infrastructure-critical cluster would need to be managed differently from standard application clusters, creating operational complexity.\n-   **Bad**, because the financial and resource cost of running a dedicated, resilient Kubernetes cluster across multiple datacenters is significantly high.\n\n### Hosted Harbor as a Service\n\n-   **Good**, because it completely eliminates the operational overhead of managing infrastructure, uptime, patching, and backups.\n-   **Good**, because it provides instant setup and access to expert support from the vendor.\n-   **Bad**, because it incurs a direct, recurring subscription cost.\n-   **Bad**, because it offers less control and flexibility over configuration and version upgrades.\n-   **Bad**, because storing images with a third party could introduce new security and data residency concerns.\n-   **Bad**, because it is still vulnerable to internet routing and network pathing issues from our datacenters.\n\n### Traditional Mutable Server Pattern\n\n-   **Good**, because the cache and all other data persist across application updates, avoiding the need for constant cache rebuilds.\n-   **Good**, because small updates can be deployed quickly without rebuilding an entire machine image.\n-   **Bad**, because it is highly susceptible to \"configuration drift,\" where servers become inconsistent and unreliable over time.\n-   **Bad**, because the state of the server is not declaratively defined, which makes troubleshooting and disaster recovery more difficult and less predictable.\n-   **Bad**, because this operational model goes against our established principle of using immutable infrastructure.\n\n## Links"},"creationDate":"2025-08-29T07:18:22.000Z","lastEditDate":"2025-08-29T07:18:22.000Z","lastEditAuthor":"Venkat","publicationDate":"2025-08-31T23:59:59.000Z","file":{"relativePath":"docs/adr/20240831-harbor-standalone-deployment-pattern.md","absolutePath":"/home/runner/work/aio-harbor-registry-cache/aio-harbor-registry-cache/docs/adr/20240831-harbor-standalone-deployment-pattern.md"},"repository":{"provider":"github","viewUrl":"https://github.com/GlueOps/aio-harbor-registry-cache/blob/master/docs/adr/20240831-harbor-standalone-deployment-pattern.md"}},"l4bVersion":"1.1.0"},"__N_SSG":true}